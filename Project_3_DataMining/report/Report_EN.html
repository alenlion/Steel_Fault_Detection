<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Mining Report - Steel Plates Fault Detection</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&family=Source+Sans+Pro:wght@300;400;600&display=swap');
        
        :root {
            --primary: #1a365d;
            --secondary: #2c5282;
            --accent: #ed8936;
            --bg: #f7fafc;
            --text: #2d3748;
            --card-bg: #ffffff;
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body {
            font-family: 'Source Sans Pro', sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: var(--text);
            line-height: 1.8;
        }
        
        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 40px 20px;
        }
        
        header {
            background: var(--card-bg);
            border-radius: 20px;
            padding: 60px 40px;
            text-align: center;
            margin-bottom: 40px;
            box-shadow: 0 25px 50px rgba(0,0,0,0.15);
        }
        
        h1 {
            font-family: 'Playfair Display', serif;
            font-size: 2.8em;
            color: var(--primary);
            margin-bottom: 15px;
        }
        
        .subtitle {
            font-size: 1.3em;
            color: var(--secondary);
            font-weight: 300;
        }
        
        .badge {
            display: inline-block;
            background: linear-gradient(135deg, var(--accent), #dd6b20);
            color: white;
            padding: 8px 20px;
            border-radius: 25px;
            margin-top: 20px;
            font-weight: 600;
        }
        
        section {
            background: var(--card-bg);
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.1);
        }
        
        h2 {
            font-family: 'Playfair Display', serif;
            color: var(--primary);
            font-size: 1.8em;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 3px solid var(--accent);
        }
        
        h3 {
            color: var(--secondary);
            font-size: 1.3em;
            margin: 25px 0 15px;
        }
        
        p { margin-bottom: 15px; }
        
        .highlight-box {
            background: linear-gradient(135deg, #ebf8ff, #bee3f8);
            border-left: 5px solid var(--secondary);
            padding: 20px 25px;
            border-radius: 0 15px 15px 0;
            margin: 20px 0;
        }
        
        .algorithm-card {
            background: #f8f9fa;
            border-radius: 15px;
            padding: 25px;
            margin: 15px 0;
            border: 1px solid #e2e8f0;
        }
        
        .algorithm-card h4 {
            color: var(--accent);
            margin-bottom: 10px;
            font-size: 1.2em;
        }
        
        .result-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }
        
        .result-item {
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            padding: 25px;
            border-radius: 15px;
            text-align: center;
        }
        
        .result-item .value {
            font-size: 2.5em;
            font-weight: 700;
            display: block;
        }
        
        .result-item .label {
            font-size: 0.9em;
            opacity: 0.9;
            margin-top: 5px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        th, td {
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid #e2e8f0;
        }
        
        th {
            background: var(--primary);
            color: white;
        }
        
        tr:hover { background: #f7fafc; }
        
        .figure {
            text-align: center;
            margin: 30px 0;
        }
        
        .figure img {
            max-width: 100%;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }
        
        .figure figcaption {
            margin-top: 15px;
            color: #718096;
            font-style: italic;
        }
        
        .code-block {
            background: #1a202c;
            color: #68d391;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Consolas', monospace;
            overflow-x: auto;
            margin: 15px 0;
        }
        
        footer {
            text-align: center;
            padding: 40px;
            color: white;
        }
        
        @media print {
            body { background: white; }
            section { box-shadow: none; border: 1px solid #ddd; }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üîµ Data Mining Report</h1>
            <p class="subtitle">Steel Plates Fault Detection - Pattern Discovery & Knowledge Extraction</p>
            <span class="badge">Project 3 - Data Mining Course</span>
        </header>

        <section>
            <h2>1. Introduction</h2>
            <p>This project applies <strong>data mining techniques</strong> to discover hidden patterns in the steel plates fault detection dataset. The goal is to understand the underlying structure of the data through exploratory analysis, dimensionality reduction, clustering, and anomaly detection.</p>
            
            <div class="highlight-box">
                <strong>üéØ Objectives:</strong>
                <ul>
                    <li>Perform comprehensive Exploratory Data Analysis (EDA)</li>
                    <li>Apply dimensionality reduction techniques (PCA, t-SNE)</li>
                    <li>Compare clustering algorithms (K-Means, Hierarchical, DBSCAN)</li>
                    <li>Detect anomalies using Isolation Forest</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>2. Dataset Description</h2>
            
            <div class="result-grid">
                <div class="result-item">
                    <span class="value">1,941</span>
                    <span class="label">Total Samples</span>
                </div>
                <div class="result-item">
                    <span class="value">27</span>
                    <span class="label">Features</span>
                </div>
                <div class="result-item">
                    <span class="value">7</span>
                    <span class="label">Fault Classes</span>
                </div>
            </div>
            
            <h3>Fault Types Distribution</h3>
            <table>
                <tr><th>Fault Type</th><th>Count</th><th>Percentage</th></tr>
                <tr><td>Other_Faults</td><td>673</td><td>34.7%</td></tr>
                <tr><td>Bumps</td><td>402</td><td>20.7%</td></tr>
                <tr><td>K_Scratch</td><td>391</td><td>20.1%</td></tr>
                <tr><td>Z_Scratch</td><td>190</td><td>9.8%</td></tr>
                <tr><td>Pastry</td><td>158</td><td>8.1%</td></tr>
                <tr><td>Stains</td><td>72</td><td>3.7%</td></tr>
                <tr><td>Dirtiness</td><td>55</td><td>2.8%</td></tr>
            </table>
            
            <div class="figure">
                <img src="../figures/class_distribution.png" alt="Class Distribution">
                <figcaption>Figure 1: Distribution of fault types in the dataset</figcaption>
            </div>
        </section>

        <section>
            <h2>3. Exploratory Data Analysis (EDA)</h2>
            
            <h3>3.1 Correlation Analysis</h3>
            <p>We computed the Pearson correlation matrix to identify relationships between features. High correlations indicate redundant features that could be combined or removed.</p>
            
            <div class="algorithm-card">
                <h4>üìä Method: Pearson Correlation</h4>
                <p><strong>Formula:</strong> r = Œ£(xi - xÃÑ)(yi - »≥) / ‚àö[Œ£(xi - xÃÑ)¬≤ √ó Œ£(yi - »≥)¬≤]</p>
                <p><strong>Why?</strong> Pearson correlation measures linear relationships between continuous variables, essential for understanding feature dependencies.</p>
            </div>
            
            <div class="figure">
                <img src="../figures/correlation_heatmap.png" alt="Correlation Heatmap">
                <figcaption>Figure 2: Correlation heatmap showing feature relationships</figcaption>
            </div>
            
            <div class="highlight-box">
                <strong>Key Findings:</strong>
                <ul>
                    <li>Strong correlation between X_Minimum and X_Maximum (geometric features)</li>
                    <li>LogOfAreas correlates highly with Pixels_Areas</li>
                    <li>Luminosity features form a distinct cluster</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>4. Dimensionality Reduction</h2>
            
            <h3>4.1 Principal Component Analysis (PCA)</h3>
            
            <div class="algorithm-card">
                <h4>üî¨ Algorithm: PCA</h4>
                <p><strong>Type:</strong> Linear dimensionality reduction</p>
                <p><strong>How it works:</strong> PCA finds orthogonal directions (principal components) that maximize variance in the data. It projects high-dimensional data onto these directions.</p>
                <p><strong>Why we used it:</strong> PCA is fast, interpretable, and provides explained variance ratios to understand information retention.</p>
            </div>
            
            <div class="result-grid">
                <div class="result-item">
                    <span class="value">43.4%</span>
                    <span class="label">Explained Variance (2D)</span>
                </div>
            </div>
            
            <div class="figure">
                <img src="../figures/pca_2d.png" alt="PCA Visualization">
                <figcaption>Figure 3: 2D PCA projection of the dataset</figcaption>
            </div>
            
            <h3>4.2 t-SNE (t-Distributed Stochastic Neighbor Embedding)</h3>
            
            <div class="algorithm-card">
                <h4>üî¨ Algorithm: t-SNE</h4>
                <p><strong>Type:</strong> Non-linear dimensionality reduction</p>
                <p><strong>How it works:</strong> t-SNE converts high-dimensional Euclidean distances to conditional probabilities and minimizes KL divergence between distributions in high and low dimensions.</p>
                <p><strong>Parameters:</strong> perplexity=30, max_iter=1000</p>
                <p><strong>Why we used it:</strong> t-SNE excels at preserving local structure and revealing clusters that linear methods miss.</p>
            </div>
            
            <div class="figure">
                <img src="../figures/tsne.png" alt="t-SNE Visualization">
                <figcaption>Figure 4: t-SNE visualization showing natural groupings</figcaption>
            </div>
        </section>

        <section>
            <h2>5. Clustering Analysis</h2>
            
            <h3>5.1 Elbow Method for Optimal K</h3>
            <p>Before applying K-Means, we used the elbow method and silhouette analysis to determine the optimal number of clusters.</p>
            
            <div class="figure">
                <img src="../figures/elbow_plot.png" alt="Elbow Plot">
                <figcaption>Figure 5: Elbow plot for K-Means clustering</figcaption>
            </div>
            
            <h3>5.2 K-Means Clustering</h3>
            
            <div class="algorithm-card">
                <h4>üéØ Algorithm: K-Means</h4>
                <p><strong>Type:</strong> Centroid-based clustering</p>
                <p><strong>How it works:</strong> Iteratively assigns points to nearest centroid and updates centroids until convergence.</p>
                <p><strong>Parameters:</strong> n_clusters=7, n_init=10, random_state=42</p>
                <p><strong>Why we used it:</strong> K-Means is simple, fast, and works well with spherical clusters.</p>
            </div>
            
            <h3>5.3 Hierarchical Clustering</h3>
            
            <div class="algorithm-card">
                <h4>üå≥ Algorithm: Agglomerative Hierarchical Clustering</h4>
                <p><strong>Type:</strong> Bottom-up clustering</p>
                <p><strong>How it works:</strong> Starts with each point as a cluster, then merges closest clusters until reaching desired number.</p>
                <p><strong>Linkage:</strong> Ward (minimizes within-cluster variance)</p>
                <p><strong>Why we used it:</strong> Produces interpretable dendrograms and doesn't require pre-specifying K.</p>
            </div>
            
            <div class="figure">
                <img src="../figures/dendrogram.png" alt="Dendrogram">
                <figcaption>Figure 6: Dendrogram showing hierarchical structure</figcaption>
            </div>
            
            <h3>5.4 DBSCAN</h3>
            
            <div class="algorithm-card">
                <h4>üîç Algorithm: DBSCAN (Density-Based Spatial Clustering)</h4>
                <p><strong>Type:</strong> Density-based clustering</p>
                <p><strong>How it works:</strong> Groups points that are closely packed, marks outliers as noise.</p>
                <p><strong>Parameters:</strong> eps=2.0, min_samples=5</p>
                <p><strong>Why we used it:</strong> DBSCAN handles arbitrary shapes and automatically detects outliers.</p>
            </div>
            
            <h3>5.5 Clustering Results Comparison</h3>
            
            <table>
                <tr>
                    <th>Method</th>
                    <th>Silhouette Score</th>
                    <th>ARI</th>
                    <th>NMI</th>
                </tr>
                <tr>
                    <td><strong>K-Means</strong></td>
                    <td>0.197</td>
                    <td>0.202</td>
                    <td>0.290</td>
                </tr>
                <tr>
                    <td>Hierarchical</td>
                    <td>0.185</td>
                    <td>0.153</td>
                    <td>0.310</td>
                </tr>
                <tr>
                    <td>DBSCAN</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
            </table>
            
            <div class="figure">
                <img src="../figures/clustering_comparison.png" alt="Clustering Comparison">
                <figcaption>Figure 7: Visual comparison of clustering results</figcaption>
            </div>
            
            <div class="highlight-box">
                <strong>üìä Evaluation Metrics:</strong>
                <ul>
                    <li><strong>Silhouette Score:</strong> Measures how similar points are to their own cluster vs. other clusters (-1 to 1, higher is better)</li>
                    <li><strong>ARI (Adjusted Rand Index):</strong> Measures agreement between clustering and true labels (-1 to 1)</li>
                    <li><strong>NMI (Normalized Mutual Information):</strong> Measures shared information between clustering and labels (0 to 1)</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>6. Anomaly Detection</h2>
            
            <div class="algorithm-card">
                <h4>üö® Algorithm: Isolation Forest</h4>
                <p><strong>Type:</strong> Tree-based anomaly detection</p>
                <p><strong>How it works:</strong> Builds random trees and isolates anomalies (they require fewer splits to isolate).</p>
                <p><strong>Parameters:</strong> contamination=0.10, random_state=42</p>
                <p><strong>Why we used it:</strong> Isolation Forest is efficient, handles high dimensions well, and requires minimal parameter tuning.</p>
            </div>
            
            <div class="result-grid">
                <div class="result-item">
                    <span class="value">194</span>
                    <span class="label">Anomalies Detected</span>
                </div>
                <div class="result-item">
                    <span class="value">10%</span>
                    <span class="label">Contamination Rate</span>
                </div>
            </div>
        </section>

        <section>
            <h2>7. Conclusions</h2>
            
            <div class="highlight-box">
                <strong>üéØ Key Findings:</strong>
                <ol>
                    <li><strong>Imbalanced Classes:</strong> Other_Faults dominates (34.7%), while Dirtiness is rare (2.8%)</li>
                    <li><strong>Feature Correlations:</strong> Strong correlations exist among geometric and luminosity features</li>
                    <li><strong>PCA Limitation:</strong> Only 43.4% variance explained in 2D, indicating complex non-linear relationships</li>
                    <li><strong>Clustering Challenge:</strong> Low silhouette scores suggest overlapping clusters</li>
                    <li><strong>K-Means Best:</strong> Among tested methods, K-Means achieved highest ARI (0.202)</li>
                </ol>
            </div>
            
            <h3>Recommendations</h3>
            <ul>
                <li>Consider feature engineering to create more separable representations</li>
                <li>Use SMOTE or other techniques to address class imbalance</li>
                <li>Investigate the 194 detected anomalies for data quality issues</li>
            </ul>
        </section>

        <section>
            <h2>8. References</h2>
            <ul>
                <li>UCI Machine Learning Repository - Steel Plates Faults Dataset</li>
                <li>Scikit-learn Documentation (https://scikit-learn.org/)</li>
                <li>van der Maaten, L., & Hinton, G. (2008). Visualizing Data using t-SNE</li>
                <li>Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). Isolation Forest</li>
            </ul>
        </section>

        <footer>
            <p>Data Mining Project - Master's Degree</p>
            <p>December 2024</p>
        </footer>
    </div>
</body>
</html>

