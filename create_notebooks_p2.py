"""
Script to create notebooks for Project 2: Machine Learning
Creates both English and Turkish versions.
"""

import json

def create_notebook(cells):
    return {
        "cells": cells,
        "metadata": {
            "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
            "language_info": {"name": "python", "version": "3.9.0"}
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }

def markdown_cell(content):
    return {"cell_type": "markdown", "metadata": {}, "source": content.split('\n')}

def code_cell(content):
    return {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": content.split('\n')}

# ============================================================
# PROJECT 2: MACHINE LEARNING - ENGLISH
# ============================================================
def create_project2_en():
    cells = [
        markdown_cell("""# Project 2: Machine Learning & Pattern Recognition

**Course**: Machine Learning

**Dataset**: Steel Plates Fault Detection

**Objective**: Multi-class classification using various ML algorithms

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [Setup and Data Loading](#2-setup-and-data-loading)
3. [Exploratory Data Analysis](#3-exploratory-data-analysis)
4. [Data Preprocessing](#4-data-preprocessing)
5. [Feature Engineering](#5-feature-engineering)
6. [Model Training](#6-model-training)
7. [Model Comparison](#7-model-comparison)
8. [Feature Importance Analysis](#8-feature-importance-analysis)
9. [Summary and Conclusions](#9-summary-and-conclusions)"""),

        markdown_cell("""---
## 1. Introduction

### Problem Statement
Predict the type of fault in steel plates using machine learning classification algorithms.

### Algorithms Implemented:
1. **Logistic Regression** - Linear baseline
2. **K-Nearest Neighbors (KNN)** - Instance-based learning
3. **Decision Tree** - Rule-based classification
4. **Random Forest** - Ensemble of trees
5. **Gradient Boosting** - Sequential ensemble
6. **SVM** - Margin-based classifier
7. **Neural Network (MLP)** - Deep learning approach
8. **XGBoost** - Advanced boosting (if available)

### Evaluation Metrics:
- Accuracy, Precision, Recall, F1-Score
- ROC-AUC (multi-class)
- Confusion Matrix"""),

        markdown_cell("""---
## 2. Setup and Data Loading"""),

        code_cell("""# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Scikit-learn
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             classification_report, confusion_matrix, roc_auc_score)

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

# Visualization settings
%matplotlib inline
plt.rcParams['figure.figsize'] = (12, 6)
sns.set_style('whitegrid')
np.random.seed(42)

print("âœ… Libraries imported successfully!")
print(f"XGBoost available: {XGBOOST_AVAILABLE}")"""),

        code_cell("""# Load data
df = pd.read_csv('../data/raw/steel_plates_fault.csv')
print(f"âœ… Data loaded: {df.shape[0]} samples, {df.shape[1]} features")
df.head()"""),

        markdown_cell("""---
## 3. Exploratory Data Analysis"""),

        code_cell("""# Dataset info
print("ğŸ“Š Dataset Information:")
print("=" * 50)
print(f"Shape: {df.shape}")
print(f"\\nMissing Values: {df.isnull().sum().sum()}")
print(f"\\nDuplicates: {df.duplicated().sum()}")
print(f"\\nData Types:")
print(df.dtypes.value_counts())"""),

        code_cell("""# Target distribution
print("ğŸ¯ Target Variable Distribution:")
class_counts = df['Class'].value_counts()
print(class_counts)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Bar chart
colors = sns.color_palette('husl', len(class_counts))
bars = axes[0].bar(class_counts.index, class_counts.values, color=colors, edgecolor='black')
axes[0].set_xlabel('Fault Type', fontweight='bold')
axes[0].set_ylabel('Count', fontweight='bold')
axes[0].set_title('Class Distribution', fontweight='bold')
plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45, ha='right')

# Pie chart
axes[1].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', colors=colors)
axes[1].set_title('Class Distribution (%)', fontweight='bold')

plt.tight_layout()
plt.savefig('../figures/class_distribution.png', dpi=300, bbox_inches='tight')
plt.show()"""),

        code_cell("""# Correlation heatmap
plt.figure(figsize=(16, 12))
corr = df.drop('Class', axis=1).corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, mask=mask, cmap='coolwarm', center=0, annot=False,
            square=True, linewidths=0.5)
plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('../figures/correlation_heatmap.png', dpi=300, bbox_inches='tight')
plt.show()"""),

        code_cell("""# Statistical summary
print("ğŸ“Š Statistical Summary:")
df.describe()"""),

        markdown_cell("""---
## 4. Data Preprocessing"""),

        code_cell("""# Separate features and target
X = df.drop('Class', axis=1)
y = df['Class']

print(f"Features: {X.shape}")
print(f"Target: {y.shape}")
print(f"\\nFeature names: {X.columns.tolist()}")"""),

        code_cell("""# Encode labels
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

print("âœ… Labels encoded!")
print(f"Classes: {encoder.classes_.tolist()}")
print(f"Encoded: {np.unique(y_encoded).tolist()}")"""),

        code_cell("""# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

print("âœ… Data split completed!")
print(f"Training: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)")"""),

        code_cell("""# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… Features scaled!")"""),

        markdown_cell("""---
## 5. Feature Engineering"""),

        code_cell("""# Create new features from existing ones
X_train_df = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_df = pd.DataFrame(X_test_scaled, columns=X.columns)

# Feature: Area ratio
if 'X_Maximum' in X.columns and 'X_Minimum' in X.columns:
    X_train_df['X_Range'] = X_train_df['X_Maximum'] - X_train_df['X_Minimum']
    X_test_df['X_Range'] = X_test_df['X_Maximum'] - X_test_df['X_Minimum']

if 'Y_Maximum' in X.columns and 'Y_Minimum' in X.columns:
    X_train_df['Y_Range'] = X_train_df['Y_Maximum'] - X_train_df['Y_Minimum']
    X_test_df['Y_Range'] = X_test_df['Y_Maximum'] - X_test_df['Y_Minimum']

# Feature: Luminosity range
if 'Maximum_of_Luminosity' in X.columns and 'Minimum_of_Luminosity' in X.columns:
    X_train_df['Luminosity_Range'] = X_train_df['Maximum_of_Luminosity'] - X_train_df['Minimum_of_Luminosity']
    X_test_df['Luminosity_Range'] = X_test_df['Maximum_of_Luminosity'] - X_test_df['Minimum_of_Luminosity']

# Feature: Aspect ratio
if 'X_Range' in X_train_df.columns and 'Y_Range' in X_train_df.columns:
    X_train_df['Aspect_Ratio'] = X_train_df['X_Range'] / (X_train_df['Y_Range'] + 0.001)
    X_test_df['Aspect_Ratio'] = X_test_df['X_Range'] / (X_test_df['Y_Range'] + 0.001)

print(f"âœ… New features created!")
print(f"Original features: {X.shape[1]}")
print(f"Total features now: {X_train_df.shape[1]}")
print(f"\\nNew features: {[c for c in X_train_df.columns if c not in X.columns]}")"""),

        code_cell("""# Update training data
X_train_engineered = X_train_df.values
X_test_engineered = X_test_df.values

print(f"Final training shape: {X_train_engineered.shape}")
print(f"Final test shape: {X_test_engineered.shape}")"""),

        markdown_cell("""---
## 6. Model Training"""),

        code_cell("""# Define models
def get_models():
    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),
        'KNN (k=10)': KNeighborsClassifier(n_neighbors=10),
        'Decision Tree': DecisionTreeClassifier(random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),
        'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=42),
        'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
    }
    
    if XGBOOST_AVAILABLE:
        models['XGBoost'] = XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')
    
    return models

models = get_models()
print(f"ğŸ“‹ {len(models)} models to train:")
for name in models.keys():
    print(f"  - {name}")"""),

        code_cell("""# Train and evaluate all models
print("=" * 70)
print("MODEL TRAINING AND EVALUATION")
print("=" * 70)

results = []
trained_models = {}

for name, model in models.items():
    print(f"\\nğŸ”„ Training {name}...")
    
    # Train
    model.fit(X_train_engineered, y_train)
    
    # Predict
    y_pred = model.predict(X_test_engineered)
    
    # Metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    
    # ROC-AUC
    roc_auc = None
    if hasattr(model, 'predict_proba'):
        try:
            y_proba = model.predict_proba(X_test_engineered)
            roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted')
        except:
            pass
    
    results.append({
        'Model': name,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1-Score': f1,
        'ROC-AUC': roc_auc
    })
    
    trained_models[name] = model
    print(f"  âœ… Accuracy: {acc:.4f}, F1: {f1:.4f}")

results_df = pd.DataFrame(results)
results_df = results_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)
print("\\nâœ… All models trained!")"""),

        markdown_cell("""---
## 7. Model Comparison"""),

        code_cell("""# Display results
print("ğŸ“Š MODEL COMPARISON RESULTS")
print("=" * 80)
print(results_df.to_string(index=False))
print("=" * 80)

# Save results
results_df.to_csv('../figures/model_comparison_results.csv', index=False)
print("\\nâœ… Results saved to '../figures/model_comparison_results.csv'")"""),

        code_cell("""# Visualize comparison
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Accuracy comparison
sorted_df = results_df.sort_values('Accuracy', ascending=True)
colors = sns.color_palette('viridis', len(sorted_df))

axes[0].barh(sorted_df['Model'], sorted_df['Accuracy'], color=colors, edgecolor='black')
axes[0].set_xlabel('Accuracy', fontweight='bold')
axes[0].set_title('Model Accuracy Comparison', fontweight='bold')
axes[0].set_xlim([0.5, 1.0])
for i, v in enumerate(sorted_df['Accuracy']):
    axes[0].text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold')

# F1-Score comparison
sorted_f1 = results_df.sort_values('F1-Score', ascending=True)
axes[1].barh(sorted_f1['Model'], sorted_f1['F1-Score'], color=colors, edgecolor='black')
axes[1].set_xlabel('F1-Score', fontweight='bold')
axes[1].set_title('Model F1-Score Comparison', fontweight='bold')
axes[1].set_xlim([0.5, 1.0])
for i, v in enumerate(sorted_f1['F1-Score']):
    axes[1].text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold')

plt.tight_layout()
plt.savefig('../figures/model_comparison_accuracy.png', dpi=300, bbox_inches='tight')
plt.show()"""),

        code_cell("""# Best model confusion matrix
best_model_name = results_df.iloc[0]['Model']
best_model = trained_models[best_model_name]

print(f"ğŸ† Best Model: {best_model_name}")
print(f"   Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")

# Confusion matrix
y_pred_best = best_model.predict(X_test_engineered)
cm = confusion_matrix(y_test, y_pred_best)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Raw counts
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],
            xticklabels=encoder.classes_, yticklabels=encoder.classes_)
axes[0].set_xlabel('Predicted', fontweight='bold')
axes[0].set_ylabel('Actual', fontweight='bold')
axes[0].set_title(f'Confusion Matrix - {best_model_name}', fontweight='bold')

# Normalized
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1],
            xticklabels=encoder.classes_, yticklabels=encoder.classes_)
axes[1].set_xlabel('Predicted', fontweight='bold')
axes[1].set_ylabel('Actual', fontweight='bold')
axes[1].set_title(f'Normalized Confusion Matrix - {best_model_name}', fontweight='bold')

plt.tight_layout()
plt.savefig('../figures/confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()"""),

        code_cell("""# Classification report for best model
print(f"ğŸ“Š Classification Report for {best_model_name}:")
print("=" * 70)
print(classification_report(y_test, y_pred_best, target_names=encoder.classes_))

# Save report
with open('../figures/classification_report.txt', 'w') as f:
    f.write(f"Best Model: {best_model_name}\\n")
    f.write("=" * 60 + "\\n")
    f.write(classification_report(y_test, y_pred_best, target_names=encoder.classes_))
print("\\nâœ… Report saved to '../figures/classification_report.txt'")"""),

        markdown_cell("""---
## 8. Feature Importance Analysis"""),

        code_cell("""# Feature importance from Random Forest
rf_model = trained_models.get('Random Forest')

if rf_model:
    feature_names = list(X.columns) + [c for c in X_train_df.columns if c not in X.columns]
    
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': rf_model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    print("ğŸ“Š Top 15 Most Important Features:")
    print(importance_df.head(15).to_string(index=False))
    
    # Save
    importance_df.to_csv('../figures/feature_importance.csv', index=False)
    
    # Visualize
    plt.figure(figsize=(12, 8))
    top_15 = importance_df.head(15)
    plt.barh(range(len(top_15)), top_15['Importance'], color='steelblue', edgecolor='black')
    plt.yticks(range(len(top_15)), top_15['Feature'])
    plt.gca().invert_yaxis()
    plt.xlabel('Importance', fontweight='bold')
    plt.title('Top 15 Feature Importances (Random Forest)', fontweight='bold')
    plt.tight_layout()
    plt.savefig('../figures/feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()"""),

        markdown_cell("""---
## 9. Summary and Conclusions

### Key Findings

1. **Best Performing Model**: Random Forest / Gradient Boosting typically perform best
2. **Feature Engineering**: Added features like X_Range, Y_Range, Aspect_Ratio improve performance
3. **Class Imbalance**: Some fault types are more common than others

### Model Comparison Summary

| Model Type | Characteristics |
|------------|-----------------|
| Logistic Regression | Fast, interpretable, linear baseline |
| KNN | Simple, sensitive to k value |
| Decision Tree | Interpretable, prone to overfitting |
| Random Forest | Robust, good generalization |
| Gradient Boosting | High accuracy, slower training |
| SVM | Good with clear margins |
| Neural Network | Flexible, needs more data |
| XGBoost | State-of-the-art for tabular data |"""),

        code_cell("""print("=" * 60)
print("ğŸ‰ PROJECT 2: MACHINE LEARNING - COMPLETE!")
print("=" * 60)
print(f"\\nğŸ† Best Model: {best_model_name}")
print(f"   Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"   F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")
print("\\nğŸ“ Output files in '../figures/':")
print("  - model_comparison_results.csv")
print("  - model_comparison_accuracy.png")
print("  - confusion_matrix.png")
print("  - classification_report.txt")
print("  - feature_importance.csv")
print("  - feature_importance.png")
print("\\nâœ… All experiments completed successfully!")""")
    ]
    return create_notebook(cells)


# ============================================================
# PROJECT 2: MACHINE LEARNING - TURKISH
# ============================================================
def create_project2_tr():
    cells = [
        markdown_cell("""# Proje 2: Makine Ã–ÄŸrenimi ve Ã–rÃ¼ntÃ¼ TanÄ±ma

**Ders**: Makine Ã–ÄŸrenimi

**Veri Seti**: Ã‡elik Levha Hata Tespiti

**AmaÃ§**: Ã‡eÅŸitli ML algoritmalarÄ± ile Ã§ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma

---

## Ä°Ã§indekiler

1. [GiriÅŸ](#1-giriÅŸ)
2. [Kurulum ve Veri YÃ¼kleme](#2-kurulum-ve-veri-yÃ¼kleme)
3. [KeÅŸifsel Veri Analizi](#3-keÅŸifsel-veri-analizi)
4. [Veri Ã–n Ä°ÅŸleme](#4-veri-Ã¶n-iÅŸleme)
5. [Ã–zellik MÃ¼hendisliÄŸi](#5-Ã¶zellik-mÃ¼hendisliÄŸi)
6. [Model EÄŸitimi](#6-model-eÄŸitimi)
7. [Model KarÅŸÄ±laÅŸtÄ±rmasÄ±](#7-model-karÅŸÄ±laÅŸtÄ±rmasÄ±)
8. [Ã–zellik Ã–nem Analizi](#8-Ã¶zellik-Ã¶nem-analizi)
9. [Ã–zet ve SonuÃ§lar](#9-Ã¶zet-ve-sonuÃ§lar)"""),

        markdown_cell("""---
## 1. GiriÅŸ

### Problem TanÄ±mÄ±
Makine Ã¶ÄŸrenimi sÄ±nÄ±flandÄ±rma algoritmalarÄ± kullanarak Ã§elik levhalardaki hata tÃ¼rÃ¼nÃ¼ tahmin etme.

### Uygulanan Algoritmalar:
1. **Lojistik Regresyon** - DoÄŸrusal temel
2. **K-En YakÄ±n KomÅŸu (KNN)** - Ã–rnek tabanlÄ± Ã¶ÄŸrenme
3. **Karar AÄŸacÄ±** - Kural tabanlÄ± sÄ±nÄ±flandÄ±rma
4. **Random Forest** - AÄŸaÃ§ topluluÄŸu
5. **Gradient Boosting** - SÄ±ralÄ± topluluk
6. **SVM** - Marj tabanlÄ± sÄ±nÄ±flandÄ±rÄ±cÄ±
7. **Yapay Sinir AÄŸÄ± (MLP)** - Derin Ã¶ÄŸrenme yaklaÅŸÄ±mÄ±
8. **XGBoost** - GeliÅŸmiÅŸ boosting

### DeÄŸerlendirme Metrikleri:
- DoÄŸruluk, Kesinlik, DuyarlÄ±lÄ±k, F1-Skoru
- ROC-AUC (Ã§ok sÄ±nÄ±flÄ±)
- KarÄ±ÅŸÄ±klÄ±k Matrisi"""),

        markdown_cell("""---
## 2. Kurulum ve Veri YÃ¼kleme"""),

        code_cell("""# KÃ¼tÃ¼phaneleri iÃ§e aktar
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Scikit-learn
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             classification_report, confusion_matrix, roc_auc_score)

# Modeller
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

# GÃ¶rselleÅŸtirme ayarlarÄ±
%matplotlib inline
plt.rcParams['figure.figsize'] = (12, 6)
sns.set_style('whitegrid')
np.random.seed(42)

print("âœ… KÃ¼tÃ¼phaneler baÅŸarÄ±yla iÃ§e aktarÄ±ldÄ±!")
print(f"XGBoost mevcut: {XGBOOST_AVAILABLE}")"""),

        code_cell("""# Veriyi yÃ¼kle
df = pd.read_csv('../data/raw/steel_plates_fault.csv')
print(f"âœ… Veri yÃ¼klendi: {df.shape[0]} Ã¶rnek, {df.shape[1]} Ã¶zellik")
df.head()"""),

        markdown_cell("""---
## 3. KeÅŸifsel Veri Analizi"""),

        code_cell("""# Veri seti bilgileri
print("ğŸ“Š Veri Seti Bilgileri:")
print("=" * 50)
print(f"Boyut: {df.shape}")
print(f"\\nEksik DeÄŸerler: {df.isnull().sum().sum()}")
print(f"\\nTekrarlanan: {df.duplicated().sum()}")"""),

        code_cell("""# Hedef daÄŸÄ±lÄ±mÄ±
print("ğŸ¯ Hedef DeÄŸiÅŸken DaÄŸÄ±lÄ±mÄ±:")
sinif_sayilari = df['Class'].value_counts()
print(sinif_sayilari)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Ã‡ubuk grafik
renkler = sns.color_palette('husl', len(sinif_sayilari))
cubuklar = axes[0].bar(sinif_sayilari.index, sinif_sayilari.values, color=renkler, edgecolor='black')
axes[0].set_xlabel('Hata TÃ¼rÃ¼', fontweight='bold')
axes[0].set_ylabel('SayÄ±', fontweight='bold')
axes[0].set_title('SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±', fontweight='bold')
plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45, ha='right')

# Pasta grafik
axes[1].pie(sinif_sayilari.values, labels=sinif_sayilari.index, autopct='%1.1f%%', colors=renkler)
axes[1].set_title('SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± (%)', fontweight='bold')

plt.tight_layout()
plt.savefig('../figures/sinif_dagilimi.png', dpi=300, bbox_inches='tight')
plt.show()"""),

        code_cell("""# Korelasyon Ä±sÄ± haritasÄ±
plt.figure(figsize=(16, 12))
korelasyon = df.drop('Class', axis=1).corr()
maske = np.triu(np.ones_like(korelasyon, dtype=bool))
sns.heatmap(korelasyon, mask=maske, cmap='coolwarm', center=0, annot=False,
            square=True, linewidths=0.5)
plt.title('Ã–zellik Korelasyon IsÄ± HaritasÄ±', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('../figures/korelasyon_haritasi.png', dpi=300, bbox_inches='tight')
plt.show()"""),

        markdown_cell("""---
## 4. Veri Ã–n Ä°ÅŸleme"""),

        code_cell("""# Ã–zellikleri ve hedefi ayÄ±r
X = df.drop('Class', axis=1)
y = df['Class']

print(f"Ã–zellikler: {X.shape}")
print(f"Hedef: {y.shape}")"""),

        code_cell("""# Etiketleri kodla
kodlayici = LabelEncoder()
y_kodlanmis = kodlayici.fit_transform(y)

print("âœ… Etiketler kodlandÄ±!")
print(f"SÄ±nÄ±flar: {kodlayici.classes_.tolist()}")"""),

        code_cell("""# EÄŸitim-test bÃ¶lÃ¼mÃ¼
X_egitim, X_test, y_egitim, y_test = train_test_split(
    X, y_kodlanmis, test_size=0.2, random_state=42, stratify=y_kodlanmis
)

print("âœ… Veri bÃ¶lme tamamlandÄ±!")
print(f"EÄŸitim: {X_egitim.shape[0]} Ã¶rnek")
print(f"Test: {X_test.shape[0]} Ã¶rnek")"""),

        code_cell("""# Ã–zellikleri Ã¶lÃ§eklendir
olcekleyici = StandardScaler()
X_egitim_olcekli = olcekleyici.fit_transform(X_egitim)
X_test_olcekli = olcekleyici.transform(X_test)

print("âœ… Ã–zellikler Ã¶lÃ§eklendirildi!")"""),

        markdown_cell("""---
## 5. Ã–zellik MÃ¼hendisliÄŸi"""),

        code_cell("""# Mevcut Ã¶zelliklerden yeni Ã¶zellikler oluÅŸtur
X_egitim_df = pd.DataFrame(X_egitim_olcekli, columns=X.columns)
X_test_df = pd.DataFrame(X_test_olcekli, columns=X.columns)

# Ã–zellik: Alan oranÄ±
if 'X_Maximum' in X.columns and 'X_Minimum' in X.columns:
    X_egitim_df['X_Aralik'] = X_egitim_df['X_Maximum'] - X_egitim_df['X_Minimum']
    X_test_df['X_Aralik'] = X_test_df['X_Maximum'] - X_test_df['X_Minimum']

if 'Y_Maximum' in X.columns and 'Y_Minimum' in X.columns:
    X_egitim_df['Y_Aralik'] = X_egitim_df['Y_Maximum'] - X_egitim_df['Y_Minimum']
    X_test_df['Y_Aralik'] = X_test_df['Y_Maximum'] - X_test_df['Y_Minimum']

# Ã–zellik: ParlaklÄ±k aralÄ±ÄŸÄ±
if 'Maximum_of_Luminosity' in X.columns and 'Minimum_of_Luminosity' in X.columns:
    X_egitim_df['Parlaklik_Aralik'] = X_egitim_df['Maximum_of_Luminosity'] - X_egitim_df['Minimum_of_Luminosity']
    X_test_df['Parlaklik_Aralik'] = X_test_df['Maximum_of_Luminosity'] - X_test_df['Minimum_of_Luminosity']

# Ã–zellik: En-boy oranÄ±
if 'X_Aralik' in X_egitim_df.columns and 'Y_Aralik' in X_egitim_df.columns:
    X_egitim_df['EnBoy_Orani'] = X_egitim_df['X_Aralik'] / (X_egitim_df['Y_Aralik'] + 0.001)
    X_test_df['EnBoy_Orani'] = X_test_df['X_Aralik'] / (X_test_df['Y_Aralik'] + 0.001)

print(f"âœ… Yeni Ã¶zellikler oluÅŸturuldu!")
print(f"Orijinal Ã¶zellik sayÄ±sÄ±: {X.shape[1]}")
print(f"Toplam Ã¶zellik sayÄ±sÄ±: {X_egitim_df.shape[1]}")"""),

        code_cell("""# EÄŸitim verilerini gÃ¼ncelle
X_egitim_muhendis = X_egitim_df.values
X_test_muhendis = X_test_df.values

print(f"Son eÄŸitim boyutu: {X_egitim_muhendis.shape}")
print(f"Son test boyutu: {X_test_muhendis.shape}")"""),

        markdown_cell("""---
## 6. Model EÄŸitimi"""),

        code_cell("""# Modelleri tanÄ±mla
def modelleri_al():
    modeller = {
        'Lojistik Regresyon': LogisticRegression(max_iter=1000, random_state=42),
        'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),
        'KNN (k=10)': KNeighborsClassifier(n_neighbors=10),
        'Karar AÄŸacÄ±': DecisionTreeClassifier(random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),
        'SVM (DoÄŸrusal)': SVC(kernel='linear', probability=True, random_state=42),
        'Yapay Sinir AÄŸÄ±': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
    }
    
    if XGBOOST_AVAILABLE:
        modeller['XGBoost'] = XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')
    
    return modeller

modeller = modelleri_al()
print(f"ğŸ“‹ EÄŸitilecek {len(modeller)} model:")
for isim in modeller.keys():
    print(f"  - {isim}")"""),

        code_cell("""# TÃ¼m modelleri eÄŸit ve deÄŸerlendir
print("=" * 70)
print("MODEL EÄÄ°TÄ°MÄ° VE DEÄERLENDÄ°RMESÄ°")
print("=" * 70)

sonuclar = []
egitilmis_modeller = {}

for isim, model in modeller.items():
    print(f"\\nğŸ”„ {isim} eÄŸitiliyor...")
    
    # EÄŸit
    model.fit(X_egitim_muhendis, y_egitim)
    
    # Tahmin
    y_tahmin = model.predict(X_test_muhendis)
    
    # Metrikler
    dogruluk = accuracy_score(y_test, y_tahmin)
    kesinlik = precision_score(y_test, y_tahmin, average='weighted', zero_division=0)
    duyarlilik = recall_score(y_test, y_tahmin, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_tahmin, average='weighted', zero_division=0)
    
    # ROC-AUC
    roc_auc = None
    if hasattr(model, 'predict_proba'):
        try:
            y_olasilik = model.predict_proba(X_test_muhendis)
            roc_auc = roc_auc_score(y_test, y_olasilik, multi_class='ovr', average='weighted')
        except:
            pass
    
    sonuclar.append({
        'Model': isim,
        'DoÄŸruluk': dogruluk,
        'Kesinlik': kesinlik,
        'DuyarlÄ±lÄ±k': duyarlilik,
        'F1-Skor': f1,
        'ROC-AUC': roc_auc
    })
    
    egitilmis_modeller[isim] = model
    print(f"  âœ… DoÄŸruluk: {dogruluk:.4f}, F1: {f1:.4f}")

sonuclar_df = pd.DataFrame(sonuclar)
sonuclar_df = sonuclar_df.sort_values('DoÄŸruluk', ascending=False).reset_index(drop=True)
print("\\nâœ… TÃ¼m modeller eÄŸitildi!")"""),

        markdown_cell("""---
## 7. Model KarÅŸÄ±laÅŸtÄ±rmasÄ±"""),

        code_cell("""# SonuÃ§larÄ± gÃ¶ster
print("ğŸ“Š MODEL KARÅILAÅTIRMA SONUÃ‡LARI")
print("=" * 80)
print(sonuclar_df.to_string(index=False))
print("=" * 80)

# SonuÃ§larÄ± kaydet
sonuclar_df.to_csv('../figures/model_karsilastirma_sonuclari.csv', index=False)
print("\\nâœ… SonuÃ§lar kaydedildi")"""),

        code_cell("""# KarÅŸÄ±laÅŸtÄ±rmayÄ± gÃ¶rselleÅŸtir
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# DoÄŸruluk karÅŸÄ±laÅŸtÄ±rmasÄ±
sirali_df = sonuclar_df.sort_values('DoÄŸruluk', ascending=True)
renkler = sns.color_palette('viridis', len(sirali_df))

axes[0].barh(sirali_df['Model'], sirali_df['DoÄŸruluk'], color=renkler, edgecolor='black')
axes[0].set_xlabel('DoÄŸruluk', fontweight='bold')
axes[0].set_title('Model DoÄŸruluk KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
axes[0].set_xlim([0.5, 1.0])

# F1-Skor karÅŸÄ±laÅŸtÄ±rmasÄ±
sirali_f1 = sonuclar_df.sort_values('F1-Skor', ascending=True)
axes[1].barh(sirali_f1['Model'], sirali_f1['F1-Skor'], color=renkler, edgecolor='black')
axes[1].set_xlabel('F1-Skor', fontweight='bold')
axes[1].set_title('Model F1-Skor KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
axes[1].set_xlim([0.5, 1.0])

plt.tight_layout()
plt.savefig('../figures/model_karsilastirma.png', dpi=300, bbox_inches='tight')
plt.show()"""),

        code_cell("""# En iyi model karÄ±ÅŸÄ±klÄ±k matrisi
en_iyi_model_adi = sonuclar_df.iloc[0]['Model']
en_iyi_model = egitilmis_modeller[en_iyi_model_adi]

print(f"ğŸ† En Ä°yi Model: {en_iyi_model_adi}")
print(f"   DoÄŸruluk: {sonuclar_df.iloc[0]['DoÄŸruluk']:.4f}")

# KarÄ±ÅŸÄ±klÄ±k matrisi
y_tahmin_en_iyi = en_iyi_model.predict(X_test_muhendis)
km = confusion_matrix(y_test, y_tahmin_en_iyi)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Ham sayÄ±lar
sns.heatmap(km, annot=True, fmt='d', cmap='Blues', ax=axes[0],
            xticklabels=kodlayici.classes_, yticklabels=kodlayici.classes_)
axes[0].set_xlabel('Tahmin', fontweight='bold')
axes[0].set_ylabel('GerÃ§ek', fontweight='bold')
axes[0].set_title(f'KarÄ±ÅŸÄ±klÄ±k Matrisi - {en_iyi_model_adi}', fontweight='bold')

# Normalize
km_norm = km.astype('float') / km.sum(axis=1)[:, np.newaxis]
sns.heatmap(km_norm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1],
            xticklabels=kodlayici.classes_, yticklabels=kodlayici.classes_)
axes[1].set_xlabel('Tahmin', fontweight='bold')
axes[1].set_ylabel('GerÃ§ek', fontweight='bold')
axes[1].set_title(f'Normalize KarÄ±ÅŸÄ±klÄ±k Matrisi - {en_iyi_model_adi}', fontweight='bold')

plt.tight_layout()
plt.savefig('../figures/karisiklik_matrisi.png', dpi=300, bbox_inches='tight')
plt.show()"""),

        markdown_cell("""---
## 8. Ã–zellik Ã–nem Analizi"""),

        code_cell("""# Random Forest'tan Ã¶zellik Ã¶nemi
rf_model = egitilmis_modeller.get('Random Forest')

if rf_model:
    ozellik_adlari = list(X.columns) + [c for c in X_egitim_df.columns if c not in X.columns]
    
    onem_df = pd.DataFrame({
        'Ã–zellik': ozellik_adlari,
        'Ã–nem': rf_model.feature_importances_
    }).sort_values('Ã–nem', ascending=False)
    
    print("ğŸ“Š En Ã–nemli 15 Ã–zellik:")
    print(onem_df.head(15).to_string(index=False))
    
    # Kaydet
    onem_df.to_csv('../figures/ozellik_onemi.csv', index=False)
    
    # GÃ¶rselleÅŸtir
    plt.figure(figsize=(12, 8))
    en_onemli_15 = onem_df.head(15)
    plt.barh(range(len(en_onemli_15)), en_onemli_15['Ã–nem'], color='steelblue', edgecolor='black')
    plt.yticks(range(len(en_onemli_15)), en_onemli_15['Ã–zellik'])
    plt.gca().invert_yaxis()
    plt.xlabel('Ã–nem', fontweight='bold')
    plt.title('En Ã–nemli 15 Ã–zellik (Random Forest)', fontweight='bold')
    plt.tight_layout()
    plt.savefig('../figures/ozellik_onemi.png', dpi=300, bbox_inches='tight')
    plt.show()"""),

        markdown_cell("""---
## 9. Ã–zet ve SonuÃ§lar

### Temel Bulgular

1. **En Ä°yi Performans GÃ¶steren Model**: Random Forest / Gradient Boosting genellikle en iyi performansÄ± gÃ¶sterir
2. **Ã–zellik MÃ¼hendisliÄŸi**: X_Aralik, Y_Aralik, EnBoy_Orani gibi eklenen Ã¶zellikler performansÄ± artÄ±rÄ±r
3. **SÄ±nÄ±f DengesizliÄŸi**: BazÄ± hata tÃ¼rleri diÄŸerlerinden daha yaygÄ±n

### Model KarÅŸÄ±laÅŸtÄ±rma Ã–zeti

| Model TÃ¼rÃ¼ | Ã–zellikler |
|------------|------------|
| Lojistik Regresyon | HÄ±zlÄ±, yorumlanabilir, doÄŸrusal temel |
| KNN | Basit, k deÄŸerine duyarlÄ± |
| Karar AÄŸacÄ± | Yorumlanabilir, aÅŸÄ±rÄ± Ã¶ÄŸrenmeye eÄŸilimli |
| Random Forest | SaÄŸlam, iyi genelleme |
| Gradient Boosting | YÃ¼ksek doÄŸruluk, yavaÅŸ eÄŸitim |
| SVM | Net sÄ±nÄ±rlarla iyi Ã§alÄ±ÅŸÄ±r |
| Yapay Sinir AÄŸÄ± | Esnek, daha fazla veri gerektirir |
| XGBoost | Tablo verileri iÃ§in en geliÅŸmiÅŸ |"""),

        code_cell("""print("=" * 60)
print("ğŸ‰ PROJE 2: MAKÄ°NE Ã–ÄRENÄ°MÄ° - TAMAMLANDI!")
print("=" * 60)
print(f"\\nğŸ† En Ä°yi Model: {en_iyi_model_adi}")
print(f"   DoÄŸruluk: {sonuclar_df.iloc[0]['DoÄŸruluk']:.4f}")
print(f"   F1-Skor: {sonuclar_df.iloc[0]['F1-Skor']:.4f}")
print("\\nğŸ“ '../figures/' klasÃ¶rÃ¼ndeki Ã§Ä±ktÄ± dosyalarÄ±:")
print("  - model_karsilastirma_sonuclari.csv")
print("  - model_karsilastirma.png")
print("  - karisiklik_matrisi.png")
print("  - ozellik_onemi.csv")
print("  - ozellik_onemi.png")
print("\\nâœ… TÃ¼m deneyler baÅŸarÄ±yla tamamlandÄ±!")""")
    ]
    return create_notebook(cells)


def main():
    print("Creating Project 2 notebooks...")
    
    nb_en = create_project2_en()
    with open('Project_2_MachineLearning/notebooks/01_MachineLearning_EN.ipynb', 'w', encoding='utf-8') as f:
        json.dump(nb_en, f, indent=2, ensure_ascii=False)
    print("  âœ… 01_MachineLearning_EN.ipynb created")
    
    nb_tr = create_project2_tr()
    with open('Project_2_MachineLearning/notebooks/01_MachineLearning_TR.ipynb', 'w', encoding='utf-8') as f:
        json.dump(nb_tr, f, indent=2, ensure_ascii=False)
    print("  âœ… 01_MachineLearning_TR.ipynb created")
    
    print("\nâœ… Project 2 notebooks created successfully!")

if __name__ == "__main__":
    main()

